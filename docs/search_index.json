[["index.html", "A Minimal Introduction to Data Management 1 Overview 1.1 General 1.2 Lectures/Discussions/Tutorials 1.3 Deliverables (Due Thursday the 9th March) 1.4 Software installation and setup", " A Minimal Introduction to Data Management Jasper Slingsby 2023-02-28 1 Overview This is a minimal introduction to data management handling data in R compiled for the Biological Sciences BSc(Honours) class at the University of Cape Town. 1.1 General I provide a very brief introduction to data management. We only have a week, so this really is a minimalist introduction. I’ll focus on providing a broad overview of the general framework and motivation for good data management (and reproducible research), teaching a few practical skills along the way. Mostly this is not fun and exciting, but it is important stuff for any biologist to know. I’ll try my best to make it interesting! Hopefully by the end of the module you’ll see the value in it all - both for you as an individual and for science and society in general. The core outcomes/concepts I hope you’ll come away with: Familiarity with the concepts and understand the need for Open, Reproducible Science Familiarity with The Data Life Cycle Some data management and handling skills 1.2 Lectures/Discussions/Tutorials These will be held live in person in BIO LT1 from 10AM to 12PM from the 4th to the 7th April unless otherwise announced on Vula. I’ll be adding to (and mostly teaching from) these online course notes as we go along. The schedule of lectures (and readings) is as follows: Monday, 27th Feb - Reproducible Research (Peng 2011; Baker 2016; Markowetz 2015) Tuesday, 28th Feb - Data Management (Michener and Jones 2012) Wednesday, 29th Feb - GitHub, Tidy Data and Data Wrangling (Wickham 2014; Wickham et al. 2019) Thursday, 30th Feb - Tidy Data and Data Wrangling continued (your turn) Friday we head off on the field trip… 1.3 Deliverables (Due Thursday the 9th March) A draft Data Management Plan (DMP) for your Honours Project using UCT’s Online DMP Tool. Please use the “University of Cape Town (UCT) - Full DMP” template. A GitHub repository containing suitably named sub-folders, data files (if small) and the R scripts (that you’ll develop based on Wednesday’s tutorial), all in line with best practice as per the content of this module. More about the R script during the Tidy data tutorial, but it must be easily executable by one of your classmates, and output your data in tidy format and a summary figure of some kind. Since you may not want your data to be public, it is best to create a private repository and invite me as a collaborator. If your data are large (&gt;5MB), then it’s best to create and only upload a smaller subset of the data. Note that your R script must still work with the reduced dataset, since part of your mark will be based on whether your script runs and the output reproducible. 1.4 Software installation and setup For the data wrangling exercise and the second deliverable, we’ll be using the R statistical programming language and the Git version control system. We’ll also be using an integrated development environment (IDE) for each: RStudio and GitHub, respectively. If you already have these installed and set up, please make sure you have the latest versions, and check that your installations are working! Please also make sure you have installed (and/or updated) the Tidyverse set of R packages. It can be installed using the code install.packages(\"tidyverse\") and updated using update.packages(\"tidyverse\"). The installation and setup can be a bit long-winded, but once done you should be good to go until you change or reformat your computer. The steps below are my summary and (hopefully) more intuitive adaptation of the instructions provided for setting up GitHub and version control with R. If my steps don’t work its probably best to read up there. First we’ll start with the necessary software. Download and install the latest version of R Download and install the latest free version of RStudio Desktop Download and install the latest version of Git - accept all the defaults Then get started with GitHub: Create a GitHub account Run through the 10 minute GitHub tutorial that is offered when you activate your GitHub Account (It’ll really help you get the idea behind what Git does!) Now you have RStudio, R and Git installed, and you have a working GitHub account that lets you do stuff online, but what remains is to get GitHub working locally and configuring RStudio to use GitHub. Install GitHub CLI (Command Line Interface). For Windows you can download the installer here Open RStudio. Select the Terminal tab (top left, next to Console) Enter gh auth login, then follow the prompts: Select GitHub.com When prompted for your preferred protocol for Git operations, select HTTPS When asked if you would like to authenticate to Git with your GitHub credentials, enter Y When asked how you would like to authenticate select Login with web browser Copy the 8-digit code and hit Enter Github.com will open in your internet browser - paste the code and hit enter If any of these steps don’t work, just start again with gh auth login in Terminal In RStudio Go to Global Options (from the Tools menu) Click Git/SVN Make sure Enable version control interface for RStudio projects is on If necessary, enter the path for your Git or SVN executable where provided (this shouldn’t be needed, but may) Click Apply Restart RStudio Lastly, you need to install the Tidyverse set of R packages. This can be done using the code install.packages(\"tidyverse\"). References "],["reproducibility.html", "2 Reproducible research 2.1 The Reproducibility Crisis 2.2 Replication and the Reproducibility Spectrum 2.3 Reproducible Scientific Workflows 2.4 Why work reproducibly? 2.5 Barriers to working reproducibly", " 2 Reproducible research This section is also available as a slideshow. Right click or hold Ctrl or Command and click this link to view in full screen. 2.1 The Reproducibility Crisis “Replication is the ultimate standard by which scientific claims are judged.” (Peng 2011) Replication is one of the fundamental tenets of science and if the results of a study or experiment cannot by replicated by an independent set of investigators then whatever scientific claims were made should be treated with caution! At best, it suggests that evidence for the claim is weak or mixed, or specific to particular ecosystems or other circumstances and cannot be generalized. At worst, there was error (or even dishonesty) in the original study and the claims were plainly false. In other words, published research should be robust enough and the methods described in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results. Sadly, this is rarely the case!!! Figure 2.1: ‘Is there a reproducibility* crisis?’ Results from a survey of &gt;1500 top scientists (Baker 2016; Penny 2016). *Note that they did not discern between reproducibility and replicability, and that the terms are often used interchangeably. We have a problem… Since we’re failing the gentleman’s agreement1 that we’ll describe our methods in enough detail that anyone else should be able to repeat the study (using the publication only) and find similar results, modern scientists are trying to formalize the process in the form of Reproducible Research. Reproducible research makes use of modern software tools to share data, code and other resources required to allow others to reproduce the same result as the original study, thus making all analyses open and transparent. Working reproducibly is not just a requirement for using quantitative approaches in iterative decision-making, it is central to scientific progress!!! 2.2 Replication and the Reproducibility Spectrum While full replication is a huge challenge (and sometimes impossible) to achieve, it is something all scientists should be working towards. Understandably, some studies may not be entirely replicable purely due to the nature of the data or phenomenon (e.g. rare phenomena, long term records, loss of species or ecosystems, or very expensive once-off science projects like space missions). In these cases the “gold standard” of full replication (from new data collection to results) cannot be achieved, and we have to settle for a lower rung on the reproducibility spectrum (Figure 2.2). Figure 2.2: The Reproducibility Spectrum (Peng 2011). Reproducibility falls short of full replication because it focuses on reproducing the same result from the same data set, rather than analyzing independently collected data. While this may seem trivial, you’d be surprised at how few studies are even reproducible, let alone replicable. 2.3 Reproducible Scientific Workflows Figure 2.3: ‘Data Pipeline’ from xkcd.com/2054, used under a CC-BY-NC 2.5 license. Working reproducibly requires careful planning and documentation of each step in your scientific workflow from planning your data collection to sharing your results. This entails a number of overlapping/intertwined components, namely: Data management - which we’ll spend more time on in Chapter 3 File and folder management Coding and code management - i.e. the data manipulation and analyses performed Computing environment and software Sharing of the data, metadata, code, publications and any other relevant materials For the rest of this section we’ll work through these components and some of the tools that help you achieve this. 2.3.1 File and folder management Project files and folders can get unwieldy fast, and can really bog you down and inhibit productivity when you don’t know where your files are or what the latest version is. Figure 2.4: ‘Documents’ from xkcd.com/1459, used under a CC-BY-NC 2.5 license. The two main considerations for addressing this issue are defining a simple, common, intuitive folder structure, and using informative file names. Folders Most ecological projects have similar requirements. Here’s a screenshot of how I usually (try to) manage my folders. “Code” we’ll deal with in the next section, but obviously contains R code etc to perform analyses. Within “Data” I often have separate folders of “Raw” and “Processed” data. If the data files are big and used across multiple projects (e.g. GIS files), then they’ll often be in a separate folder elsewhere on my computer, but this is well-documented in my code. “Output” contains figures and tables, often in separate folders. I also often have a “Manuscript” folder if I’m working in LaTeX/Sweave or RMarkdown, although this is often in the “Code” folder (since you can embed code in RMarkdown and Sweave documents). File and folder naming Your naming conventions should be: machine readable i.e. avoid spaces and funny punctuation support searching and splitting of names (e.g. “data_raw_CO2.csv”, “data_clean_CO2.csv”, “data_raw_species.csv” can all be searched by keywords and can be split by “_” into 3 useful fields: type (data vs other), class (raw vs clean), variable (CO2 vs species), etc) human readable the contents should be self evident from the file name support sorting i.e. use numeric or character prefixes to separate files into different components or steps (e.g. “data_raw_localities.csv”, “data_clean_localities.csv”, etc) some of this can be handled with folder structure, but you don’t want too many folders either Find out more about file naming here. 2.3.2 Coding and code management Why write code? Working in point-and-click GUI-based software like Excel, Statistica, SPSS, etc may seem easier, but you’ll regret it in the long run… The beauty of writing code lies in: Automation You will inevitably have to adjust and repeat your analysis as you get feedback from supervisors, collaborators and reviewers. Rerunning code is one click, and you’re unlikely to introduce errors. Rerunning analyses in GUI-based software is lots of clicks and it’s easy to make mistakes, alter default settings, etc etc. Next time you need to do the same analysis on a different dataset you can just copy, paste and tweak your code. You code/script provides a record of your analysis Linked to the above, mature scientific coding languages like Python or R allow you to run almost any kind of analysis in one scripted workflow, even if it has diverse components like GIS, phylogenetics, multivariate or Bayesian statistics, etc. Most proprietary software are limited to one or a few specialized areas (e.g. ArcGIS, etc), which leaves you manually exporting and importing data between multiple software packages. This is very cumbersome, in addition to being a file-management nightmare… Most scripting environments are open source (e.g. R, Python, JavaScript, etc) Anyone wanting to use your code doesn’t have to pay for a software license It’s great for transparency - Lots of people can and have checked the background code and functions you’re using, versus only the software owner’s employees have access to the raw code for most analytical software There’s usually a culture of sharing code (online forums, with publications, etc) Here’s a motivation and some tutorials to help you learn R. Some coding rules It’s easy to write messy code. This can make it virtually indecipherable to others (and even yourself), slowing you and your collaborations down. It also makes it easy to make mistakes and not notice them. The overarching rule is to write code for people, not computers. Check out the Tidyverse style guide for R-specific guidance, but here are some basic rules: use consistent, meaningful and distinct names for variables and functions use consistent code and formatting style use commenting to document and explain what you’re doing at each step or in each function - purpose, inputs and outputs “notebooks” like RMarkdown or Jupyter Notebooks are very handy for fulfilling roles like documentation, master/makefiles etc and can be developed into reports or manuscripts write functions rather than repeating the same code modularize code into manageable steps/chunks or even separate them into separate scripts that can all be called in order from a master script or Makefile check for mistakes at every step!!! Beyond errors or warnings, do the outputs make sense? start with a “recipe” that outlines the steps/modules (usually as commented headers etc). This is very valuable for keeping you organized and on track, e.g. a common recipe in R: #Header indicating purpose, author, date, version etc #Define settings #Load required libraries #Read in data #Wrangle/reformat/clean/summarize data as required #Run analyses (often multiple steps) #Wrangle/reformat/summarize analysis outputs for visualization #Visualize outputs as figures or tables avoid proprietary formats i.e. use an open source scripting langauge and open source file formats only use version control!!! Version control Using version control tools like Git, SVN, etc can be challenging at first, but they can also hugely simplify your code development (and adaptation) process. While they were designed by software developers for software development, they are hugely useful for quantitative biology. I can’t speak authoritatively on version control systems (I’ve only ever used Git and GitHub), but here are the advantages as I see them. This version is specific to Git, but I imagine they all have similar functions and functionality: Words in italics are technical terms used within GitHub. You can look them up here. You’ll also cover it in the brief tutorial you’ll do when setting up your computer for the practical. They generally help project management, especially collaborations They allow you to easily share code with collaborators or the public at large - through repositories or gists (code snippets) Users can easily adapt or build on each others’ code by forking repositories and working on their own branch. This is truly powerful!!! It allows you to repeat/replicate analyses but even build websites (like this one!), etc While the whole system is online, you can also work offline by cloning the repository to your local machine. Once you have a local version you can push to or pull from the online repository to keep everything updated Changes are tracked and reversible through commits. If you change the contents of a repository you must commit them and write a commit message before pulling or pushing to the online repository. Each commit is essentially a recoverable version that can be compared or reverted to This is the essence of version control and magically frees you from folders full of lists of files named “mycode_final.R”, “mycode_finalfinal.R”, “myfinalcode_finalfinal.R” etc as per Figure 2.4 They allow collaborators or the public at large to propose changes via pull requests that allow you to merge their forked branch back to the main (or master) branch They allow you to accept and integrate changes seamlessly when you accept and merge pull requests They allow you to keep written record of changes through comments whenever a commit or pull request is made - these also track the user, date, time, etc and are useful for blaming when things go wrong There’s a system for assigning logging and tracking issues and feature requests I’m sure this is all a bit much right now, but should make more sense after the practical… 2.3.3 Computing environment and software We’ve already covered why you should use open source software whenever possible, but it bears repeating. Using proprietary software means that others have to purchase software, licenses, etc to build on your work and essentially makes it not reproducible by putting it behind a pay-wall. This is self-defeating… Another issue is that software and hardware change with upgrades, new versions or changes in the preferences within user communities (e.g. you’ll all know MicroSoft Excel, but have you heard of Quattro Pro or Lotus that were the preferred spreadsheet software of yesteryear?). Just sharing your code, data and workflow does not make your work reproducible if we don’t know what language the code is written in or if functions change or are deprecated in newer versions, breaking your code. The simplest way to avert this problem is to carefully document the hardware and versions of software used in your analyses so that others can recreate that computing environment if needed. This is very easy in R, because you can simply run the sessionInfo() function, like so: sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: aarch64-apple-darwin20 (64-bit) ## Running under: macOS Ventura 13.0 ## ## Matrix products: default ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] googlesheets4_1.0.1 readxl_1.4.2 forcats_0.5.2 stringr_1.5.0 dplyr_1.0.10 ## [6] purrr_1.0.1 readr_2.1.3 tidyr_1.3.0 tibble_3.1.8 ggplot2_3.4.0 ## [11] tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.4 sass_0.4.5 jsonlite_1.8.4 modelr_0.1.10 bslib_0.4.2 assertthat_0.2.1 ## [7] highr_0.10 cellranger_1.1.0 yaml_2.3.7 pillar_1.8.1 backports_1.4.1 glue_1.6.2 ## [13] digest_0.6.31 RColorBrewer_1.1-3 promises_1.2.0.1 rvest_1.0.3 colorspace_2.1-0 htmltools_0.5.4 ## [19] httpuv_1.6.9 plyr_1.8.8 pkgconfig_2.0.3 broom_1.0.3 haven_2.5.1 bookdown_0.32 ## [25] scales_1.2.1 jpeg_0.1-10 later_1.3.0 tzdb_0.3.0 timechange_0.2.0 googledrive_2.0.0 ## [31] generics_0.1.3 farver_2.1.1 ellipsis_0.3.2 cachem_1.0.6 withr_2.5.0 cli_3.6.0 ## [37] magrittr_2.0.3 crayon_1.5.2 evaluate_0.20 GGally_2.1.2 fs_1.6.0 fansi_1.0.4 ## [43] xml2_1.3.3 tools_4.2.2 hms_1.1.2 gargle_1.2.1 lifecycle_1.0.3 munsell_0.5.0 ## [49] reprex_2.0.2 compiler_4.2.2 jquerylib_0.1.4 rlang_1.0.6 grid_4.2.2 rstudioapi_0.14 ## [55] rappdirs_0.3.3 labeling_0.4.2 rmarkdown_2.20 gtable_0.3.1 DBI_1.1.3 reshape_0.8.9 ## [61] curl_5.0.0 R6_2.5.1 lubridate_1.9.1 knitr_1.42 fastmap_1.1.0 utf8_1.2.2 ## [67] stringi_1.7.12 Rcpp_1.0.10 vctrs_0.5.2 png_0.1-8 dbplyr_2.3.0 tidyselect_1.2.0 ## [73] xfun_0.36 Containers A “better” way to do this is to use containers like docker or singularity. These are contained, lightweight computing environments similar to virtual machines, that you can package with your software/workflow. You set your container up to have everything you need to run your code etc (and nothing extra), so anyone can download (or clone) your container, code and data and run your analyses perfectly first time. 2.3.4 Sharing of the data, code, publication etc This is touched on in more detail when we discuss data management in Chapter 3, but suffice to say there’s no point working reproducibly if you’re not going to share all the components necessary to complete your workflow… Another key component here is that ideally all your data, code, publication etc are shared Open Access - i.e. they are not stuck behind some paywall Figure 2.5: A 3-step, 10-point checklist to guide researchers toward greater reproducibility in their research (Alston and Rick 2021). 2.4 Why work reproducibly? Figure 2.6: Let’s start being more specific about our miracles… Cartoon © Sidney Harris. Used with permission ScienceCartoonsPlus.com In addition to basic scientific rigour, working reproducibly is hugely valuable, because: (Adapted from “Five selfish reasons to work reproducibly” (Markowetz 2015)) Its transparent and open, helping us avoid mistakes and/or track down errors in analyses This is what highlighted the importance of working reproducibly for me. In 2017 I published the first evidence of observed climate change impacts on biodiversity in the Fynbos Biome (Slingsby et al. 2017). The analyses were quite complicated, and when working on the revisions I found an error in my R code. Fortunately, it didn’t change the results qualitatively, but it made me realize how easy it is to make a mistake and potentially put the wrong message out there! This encouraged me to make all data and R code from the paper available, so that anyone is free to check my data and analyses and let me (and/or the world) know if they find any errors. It makes it easier to write papers e.g. Dynamic documents like RMarkdown or Jupyter Notebooks update automatically when you change your analyses, so you don’t have to copy/paste or save/insert all tables and figures - or worry about whether you included the latest versions. It helps the review process Often issues picked at by reviewers are matters of clarity/confusion. Sharing your data and analyses allows them to see exactly what you did, not just what you said you did, allowing them to identify the problem and make constructive suggestions. It’s also handy to be able to respond to a reviewer’s comment with something like: “That’s a great suggestion, but not really in line with the objectives of the study. We have chosen not to include the suggested analysis, but do provide all data and code so that interested readers can explore this for themselves.” (Feel free to copy and paste - CCO 1.0) It enables continuity of the research When people leave a project (e.g. students/postdocs), or you forget what you did X days/weeks/months/years ago, it can be a serious setback for a project and make it difficult for you or a new student to pick up where things left off. If the data and workflow are well curated and documented this problem is avoided. Trust me, this is a very common problem!!! I have many papers that I (or my students) never published and may never go back to, because I know it’ll take me a few days or weeks to understand the datasets and analyses again… This is obviously incredibly important for long-term projects! A little bit of extra effort early on can save a lot of time further down the road!!! It helps to build your reputation Working reproducibly makes it clear you’re an honest, open, careful and transparent researcher, and should errors be found in your work you’re unlikely to be accused of dishonesty (e.g. see my paper example under point 1 - although no one has told me of any errors yet…). When others reuse your data, code, etc you’re likely to get credit for it - either just informally, or formally through citations or acknowledgements (depending on the licensing conditions you specify - see “Preserve” in the Data Life Cycle). And some less selfish reasons (and relevant for ecoforecasting): It speeds progress in science by allowing you (or others) to rapidly build on previous findings and analyses Somewhat linked to point 4, but here the focus is on building on published work. For example, if I read a paper and have an idea (or develop a new hypothesis) that may explain some of their results or add to the story, I can start from where they left off rather than collecting new data, recoding their whole analysis, etc before I can even start. It allows easy comparison of new analytical approaches (methods, models, etc) to older ones Linked to 6, but more specific to model or methods development where the need to “benchmark” your new method relative to an older one is important. If the benchmark model exists you can make an honest comparison, but if you have to set up the older one yourself, some may accuse you of gaming the system by chosing settings etc that advantage your new method. It makes it easy to repeat the same analyses when new data are collected or added This is key for iterative forecasting, but also useful if you want to apply the same method/model to a new data set (e.g. applying Merow et al. (2014)’s analysis on all 26 Proteaceae studied by Treurnicht et al. (2016)). Most skills and software used in Reproducible Research are very useful beyond Reproducible Research alone! e.g. GitHub - in addition to versioning code etc is great for code management, collaborative projects and can be used for all kinds of things like building websites (e.g. these course notes). And one more selfish reason (but don’t tell anyone I said this): Reproducible research skills are highly sought after in careers like data science etc… Skills are important should you decide to leave biology… Even within biology, more and more environmental organizations and NGOs are hiring data scientists or scientists with strong data and quantitative skills. Some examples I know of: The South African Environmental Observation Network (SAEON - especially their data node uLwazi) The Endangered Wildlife Trust (EWT) The Nature Conservancy 2.5 Barriers to working reproducibly (Adapted from “A Beginner’s Guide to Conducting Reproducible Research” (Alston and Rick 2021)) 1. Complexity There can be a bit of a learning curve in getting to know and use the tools for reproducible research effectively. One is always tempted by the “easy option” of doing it the way you already know or using “user-friendly” proprietary software. 2. Technological change Hardware and software used in analyses change over time - either changing with updates or going obsolete altogether - making it very difficult to rerun old analyses. This should be less of a problem going forward because: it is something people are now aware of (so we’re working on solutions) we’re increasingly using open source software, for which older versions are usually still made available and there is little risk of it disappearing when the software company stops supporting the software or goes bankrupt documenting hardware and software versions with analyses is an easy baseline increasingly people are using contained computing environments as we’ll discuss below 3. Human error Simple mistakes or failure to fully document protocols or analyses can easily make a study irreproducible. Most reproducible research tools are aimed at solving this problem. 4. Intellectual property rights Rational self-interest can lead to hesitation to share data and code via many pathways: Fear of not getting credit; Concern that the materials shared will be used incorrectly or unethically; etc Hopefully most of these issues will be solved by better awareness of licensing issues, attribution, etc, as the culture of reproducible research grows References "],["data.html", "3 Data Management 3.1 Why do you need to manage your data? 3.2 The Data Life Cycle 3.3 Data and decisions", " 3 Data Management 3.1 Why do you need to manage your data? Data management is often the last thing on a scientists mind when doing a new study - “I have a cool idea, and I’m going to test it!”. You don’t want to “waste” time planning how you’re going to manage your data and implementing that plan… Unfortunately, this never ends well and really is a realm where “haste makes waste”. Figure 3.1: The ‘Data Decay Curve’ (Michener et al. 1997) Here are a bunch of reasons you really want to focus on doing good data management: Bad data management leads to data loss… (Figure 3.1) Your future self will hate you if you lose it before you’re finished with it!!! This is less likely in the world of Dropbox, Google Drive, iCloud etc, but I know people who had to repeat their PhD’s because they lost their data because it was on a laptop that crashed or was stolen… Also, beware cloud storage!!! It’s very easy for you or a collaborator to delete/overwrite/lose access to items, e.g.  if someone leaves the project and deletes the folder on their Dropbox without “leaving” it first if the “owner” of the Google Drive folder loses access to their Google account (as will happen to your UCT Google Drive access as soon as you graduate!!!) through all manner of random “accidents” Data has value beyond your current project: to yourself for reuse in future projects, collaborations, etc (i.e. publications and citations), for others for follow-up studies, or combining multiple datasets for meta-analyses or synthesis for science on general (especially long-term ecology in a time of global change) We’ve covered this before, but sharing your data is key for transparency and accountability. Data collection is expensive, and is often paid for with taxpayers’ money. You owe it to your funder (and humanity in general) to make sure that science gets the most out of your data in the long term. Lastly, good planning and data management can help iron out issues up front, like: intellectual property, permissions for ethics, collection permits, etc, outlining expectations for who will be authors on the paper(s), responsibilities for managing different aspects of the data etc If you don’t establish these permissions and ground rules early they can result in data loss, not being able to publish the study, damage relationships in collaborations (including student-supervisor), and ultimately damage careers… To avoid data (and relationship) decay, and to reap the benefits of good data management, it is important to consider the full Data Life Cycle. 3.2 The Data Life Cycle Figure 3.2: The Data Life Cycle, adapted from https://www.dataone.org/ Note that there are quite a few different versions of the data life cycle out there. This is the most comprehensive one I know of, and covers all the steps relevant to a range of different kinds of research projects. A full description of this data life cycle and related ecoinformatics issues can be found in (Michener and Jones 2012). Not all projects need to do all steps, nor will they necessarily follow the order here, but it is worth being aware of and considering all steps. For example: Often the first thing you do when you have a new hypothesis is start by searching for any existing data that could be used to test it without having to spend money and time collecting new data (i.e. skip to step 6 - “Discover”). In this case I would argue that you should still do step 1 (Plan), and you’d want to do some checking to assure the quality of the data (step 3), but you can certainly skip steps 2, 4 and 5. A meta-analysis or synthesis paper would probably do the same. If you’re collecting new data you would do steps 1 to 5 and possibly skip 6 and 7, although in my experience few studies do not reuse existing data (e.g. weather or various GIS data to put your new samples into context). 3.2.1 Plan Good data management begins with planning. In this step you essentially outline the plan for every step of the cycle in as much detail as possible. Usually this is done by constructing a document or Data Management Plan (DMP). While developing DMPs can seem tedious, they are essential for the reasons I gave above, and because most funders and universities now require them. Fortunately, there are a number of online data management planning tools that make it easy by providing templates and prompts to ensure that you cover all the bases, like the Digital Curation Centre’s DMPOnline and UCT’s DMP Tool. Figure 3.3: Screenshot of UCT’s Data Management Planning Tool’s Data Management Checklist. A key thing to bear in mind is that a DMP is a living document and should be regularly revised during the life of a project, especially when big changes happen - e.g. new team members, new funding, new direction, change of institution, etc. I typically develop one overarching DMP for an umbrella project (e.g. a particular grant), but then add specifics for subprojects (e.g. separate student projects etc). 3.2.2 Collect and Assure There are many, many different kinds of data that can be collected in a vast number of ways! Figure 3.4: Springer Nature Infographic illustrating the vast range of research data types. While “Collect” and “Assure” are different steps in the life cycle, I advocate that it is foolish to collect data without doing quality assurance and quality control (QA/QC) as you go, irrespective of how you are collecting the data. For example: automated logging instruments (weather stations, cameras, acoustic recorders) need to be checked that they’re logging properly, are calibrated/focused, are reporting sensible values, etc if you’re filling in data sheets, you need to check that all fields have been completed (no gaps), that there are no obvious errors and that any numbers or other values look realistic. In fact, if you’re using handwritten data sheets it’s best to capture them as soon as possible (i.e. that evening), because that helps you spot errors and omissions, you have a better chance of deciphering bad handwriting or cryptic notes, and you can plot any values to see if there are suspicious outliers (e.g. because someone wrote down a measurement in centimetres when they were meant to use metres). When transcribing or capturing data into a spreadsheet or database it is often best to use data validation tricks like drop-down menus, conditional formatting, restricted value ranges etc to avoid spelling mistakes and highlight data entries that are outside the expected range of the data field. It may seem like a lot of effort to set this up, but it’ll save you a lot of time and pain in the long run!!! Increasingly, I’ve started moving towards capturing data directly into a spreadsheet with data validation rules using a phone or tablet. There are also a number of “no code” app builders these days like AppSheet that allows you to easily collect data on your phone and sync data directly into Google Sheets and photos to your Google Drive. AppSheet is proprietary software, but does allow a lot of utility with their free accounts. QField is another handy open source app built on QGIS, allowing you to setup maps and forms in QGIS on your workstation and deploy them to your phone, tablet etc. It seems to be getting better all the time. Figure 3.5: An example data collection app I built in AppSheet that allows you to log GPS coordinates, take photos, record various fields, etc. Tidy Data Last, but not least. We haven’t discussed different data formats etc, but if you are working with tabular data (i.e. spreadsheets, tables) I strongly recommend you read this short paper on how to keep your data Tidy (Wickham 2014). Following these principles will make life much easier for you once you get to the analysis step… 3.2.3 Describe There are few things worse than having a spreadsheet of what you think is the data you need, but you don’t know what the column names mean, how variables were measured, what units they’re reported in, etc… - Especially when you were the one who collected and captured the data!!! This descriptive data about the data is called metadata and is essential for making the data reusable, but is also useful for many other purposes like making the data findable (e.g. using keyword searches). In fact, metadata makes up the majority of what are called the FAIR data principles (Wilkinson et al. 2016), which largely focus on this and the next few steps of the Data Life Cycle. I’m not going to dwell on them other than to say that they are a key component of making your work reproducible, and that like reproducibility, practicing FAIR data principles is a spectrum. Figure 3.6: The FAIR data principles ErrantScience.com. Some key kinds of metadata: the study context why the data were generated who funded, created, collected, assured, managed and owns the data (not always the same person) contact details for the above when and where the data were collected where the data are stored the data format what is the file format what softwares were used (and what version) the data content what was measured how it was measured what the columns and rows are what units it’s reported in what QA/QC has been applied is it raw data or a derived data product (e.g. spatially interpolated climate layers) if derived, how it was analyzed etc Metadata standards and interoperability Many data user communities have developed particular metadata standards or schemas in an attempt to enable the best possible description and interoperability of a data type for their needs. They are typically human and machine-readable data, so that the metadata records can also be read by machines, to facilitate storing and querying multiple datasets in a common database (or across databases). Imagine how difficult it would be to pay for something electronically if banks didn’t use common metadata standards? Each transaction would require someone to manually look things up in multiple tables etc etc. Chaos!!! Figure 3.7: How standards proliferate… from xkcd.com/927, used under a CC-BY-NC 2.5 license. Using common metadata schemas has many advantages in that they make data sharing easier, they allow you to search and integrate data across datasets, and they simplify metadata capture (i.e. having a list of required fields makes it easier to not forget any). There are many standards, but perhaps the most common ones you’ll encounter in biological sciences (other than geospatial metadata standards) are DarwinCore and Ecological Metadata Language (EML). There’s even new standards for documenting ecological forecasts! Figure 3.8: An example of a geospatial metadata standard. SpatioTemporal Asset Catalogs (STAC; stacspec.org) aims to provide a common specification to enable online search and discovery of geospatial assets in just about any format. 3.2.4 Preserve There are two major components to preserving your data: Back your data up now!!! (and repeat regularly) Losing your data can be incredibly inconvenient!!! A good friend of mine lost all of his PhD data twice. It took him 7 years to complete the degree… Beyond inconvenience, losing data can be incredibly expensive! Doing 4 extra years to get your PhD is expensive at a personal level, but if the data are part of a big project it can rapidly add up to millions - like How Toy Story 2 Almost Got Deleted. PRO TIP: Storing data on the cloud is not enough! You could easily delete that single version of all your data! You may also lose access when you change institution etc. E.g. What happens to your UCT MS OneDrive and Google Drive content when you graduate and ICTS close your email account? Long-term preservation and publication This involves the deposition of your data (and metadata!) in a data repository where it can be managed and curated over the long term. This is increasingly a requirement of funders and publishers (i.e. journals). Many journals allow you (or require you) to submit and publish your data with them as supplementary material. Unfortunately, many journals differ in how they curate the data and whether they are available open access. I prefer to publish my data in an online open access repository where you can get a permanent Digital Object Identifier (DOI) that you can link to from your paper. Another consideration, if you are keen for people to reuse your data (which if you are not you will fail this course by default) is where people are most likely to look for your data (i.e. making your data “Findable/Discoverable”). There are many “bespoke” discipline-specific data repositories for different kinds of data, e.g. Global databases: GenBank - for molecular data TRY - for plant traits Dryad - for generalist biological and environmental research South African databases: SANBI - for most kinds of South African biodiversity data SAEON - for South African environmental data (e.g. hydrology, meteorology, etc) and biodiversity data that don’t fit SANBI’s databases If none of these suit your data, there are also “generalist” data repositories that accept almost any kind of data, like: FigShare Zenodo UCT’s ZivaHub (which is built on and searchable through FigShare) I haven’t discussed physical samples at all. These are obviously a huge (if not bigger) challenge too, although there are some obvious homes for common biological data, like herbaria for plant collections and museums for animal specimens. 3.2.5 Discover This is perhaps the main point of the Data Life Cycle and FAIR data principles - to make data findable so that it can be reused. The biggest challenge to discovering data is that so many datasets are not online and are in the “filing cabinet in a bath in the basement under a leaking pipe” as in Figure 3.6. If you preserve and publish them in an online data repository, this overcomes the biggest hurdle. The next biggest challenge is that there is so much online that finding what you need can be quite challenging (like looking for a needle in a haystack…). This is where choosing the right portal can be important. It is also what metadata standards are aimed at - allowing interoperable searches for specific data types across multiple repositories. A final consideration is whether you have permission to use the data. You can often find out about the existence of a dataset, either online or in a paper, but the data aren’t made freely available. This is where licensing comes into play. Most data repositories require you to publish the data under a license. There are many options depending on the kind of data and what restrictions you want to put on its use. I’m not going to go into the gory details, but Creative Commons have created an extensible system of generic licenses that are easy to interpret and cover most situations. I say extensible because the licenses are made up of a string of components that can be layered over each other. For example: CCO - means it is Open - i.e. there are no restrictions on use and it is in the public domain CC BY - means by attribution - you can use the data for any purpose, but only if you indicate attribution of the data to the source or owner of the data CC BY-SA - means by attribution + share alike - i.e. you have to indicate attribution and share your derived product under the same license CC BY-ND - means by attribution + no derivatives - i.e. you have to indicate attribution, but cannot use it to make a derived product. This is often used for images - allowing you to show the image, but not to alter it. CC BY-NC - means by attribution + non-commercial - you have to indicate attribution, but cannot use it for commercial purposes (i.e. you can’t sell derived products) CC BY-NC-SA - by attribution + non-commercial + share alike CC BY-NC-ND - by attribution + non-commercial + no derivatives NOTE: As an aside, for some reason software licenses are a bit more complicated and code is rarely shared under CC licenses, other than CCO. A rough equivalent of CC BY for code is MIT, and there are others that can add various constraints similar to Creative Commons’ SA, ND and NC. See here. 3.2.6 Integrate There are a few different components to data integration in this context: Linking different kinds of data, usually through spatial and or temporal information e.g. matching you biodiversity collections with weather records or GIS information about the sites Keeping track of changes you’ve made to your data as you prepare it for analyses (versioning) e.g. you may be wanting to compare species richness across sites. This requires estimating species richness from your field data (usually lists of species by site occurrences and/or abundances) you should always keep a copy of your raw data!!! using scripting languages for data handling and analyses (e.g. R, Python, MatLab) can help you keep record of how you did any data summaries, transformations, etc, but only if you write clean, well-documented code and manage your code well!!! Curating your data such that they can easily be integrated with other, similar datasets for larger analyses or meta-analyses this is largely a metadata game, but also one of data formats etc. Many fields promote the use of common data standards with rules on measurement specifications, file formats, common data and metadata fields, controlled vocabularies etc that allow easy integration, searching and manipulation (see section 3.2.3 for more details). This is what a lot of the discipline-specific online databases attempt to achieve. 3.2.7 Analyze “The fun bit”, but again, there are many things to bear in mind and keep track of so that your analysis is repeatable. This is largely covered by the sections on Coding and code management and Computing environment and software in Chapter 2 3.3 Data and decisions This is not part of The Data Life Cycle per se, but it’s worth remembering that there are some other aspects of data, while still important if the goal is purely academic, that are make-or-break when the goal is informing decisions, e.g. (mostly paraphrased from Dietze et al. (2018)): Latency - the time between data collection and it’s availability for modelling. Depending on the model’s need for the latest data, if the latency is too long then it can preclude the ability to make useful forecasts (e.g. it’s very difficult to make useful weekly forecasts if you only get new data once a year…). Uncertainty - the model can only ever be as good as the data (GIGO: garbage in = garbage out). We’ll also see that assimilating data into forecasts requires uncertainty estimates. Not including uncertainty can create bias or overconfident predictions… Unfortunately, very rarely does anyone report the uncertainty in their data… Unfortunately, there are also many reasons why data can never be certain - sampling variability, random and systematic instrument errors, calibration uncertainty, classification errors (e.g. species identification), transcription errors, corrupt files (or collaborators!) etc. I live by the creed there is nothing as sinister as a certainty. If you tell me your model has an \\(R^2\\) of 1, I will tell you (with 100% certainty, ironically) that your model is wrong. Repeated sampling - most forecasts are in time and thus require time-series to develop the models. Frequent repeated sampling can often come at a trade-off with the spatial extent (or spatial replication) of sampling though. Forecasters need to optimize this trade-off to inform the most efficient data collection while also reducing uncertainty in the model. Interoperability - this is largely covered under the Describe, Preserve, Discover and Integrate steps in The Data Life Cycle References "],["tidy-data.html", "4 Tidy Data 4.1 Data entry 4.2 Reproducibility - start as you mean to finish 4.3 Tidy data and data wrangling 4.4 Reading in data 4.5 A quick aside on data types 4.6 Learning to speak Tidyverse 4.7 Getting things Tidy 4.8 Getting out of the tidyverse 4.9 Joining dataframes", " 4 Tidy Data This section is aimed at helping you develop a relatively simple reproducible workflow for your project (i.e. Deliverable 2 described in section 1.3\\(^*\\)) and providing a demonstration of some of the Tidyverse tools for wrangling and tidying your data. \\(^*\\)But note that the content of the previous 2 sections are important for the deliverable too. But first you need to get your data into a digital format… 4.1 Data entry NOTE: This section may not be relevant to the module deliverable, because you should have an example dataset that’s already in digital format, but it will be useful if/when you collect new data of your own. Before we start, I should highlight that one very good way of reducing errors in data entry and improving quality assurance and quality control (QA/QC) is to make use of various forms of data validation like drop down lists and conditional formatting. These can be used to limit data types (i.e. avoids you entering text into a numeric column or vice versa), or limiting the range of values entered (e.g. 0 to 100 for percentages), or just highlighting values that are possible, but rare, and should be double-checked, like 100m tall trees. Unit errors are incredibly common in ecological data entry, especially for things like plant height where you may be measuring 20cm tall grasses and 20m tall trees!!! Drop down lists really come into their own when you’re working with lots of different Latin species names etc. Typos, spelling errors and synonyms can take weeks of work to fix! Restricting the names to a drop-down list from a known published taxonomic resource really makes a huge difference! Here’s an example from a vegetation survey practical I ran as part of the BIO3018F Ecology and Evolution 3rd-year field course. If you go to the Sites sheet, you’ll see that the first two columns Site and Point are drop-down lists that reference the RangeLimits spreadsheet, to make entry easy and to prevent spelling mistakes etc. You’ll also notice that the different columns have specified ranges, and that a number outside the range is entered (e.g. anything &gt;100 for PercentBareSoil) then the cell is highlighted with a warning. You can see the data validation rules by selecting a cell in the column of interest and clicking Data &gt; Data validation It’s often easy to forget to collect some measurements etc when in the field, like site-level information. Sometimes what I do to prevent this is set up the next data entry sheet (trait data in this case), to require that you fill in the site name from a drop-down list that is populated in the site data sheet. This limits the names you can use to only those that have site data, and serves as a reminder should you have forgotten. Check out the SitePoint column in the Traits sheet. I’m not going to go into how to do data validation in spreadsheets, but here are links for: Microsoft Excel Google Sheets I’m sure there are plenty of other online resources too. One thing I really like about Google Sheets is that you can set your sheet to work in “offline mode” on your phone or tablet, which allows you to do data entry directly into the sheet in the field, which then syncs to your Google Drive when you go back online (I’d imagine you can do the same with MS Excel and One Drive, but I’ve never tried it). This really saves time (and errors) on entering written field data sheets into a spreadsheet later, and allows you to take advantage of the data validation rules in the field, which are harder to set up with pen or pencil… The only issue is that if you make a typo etc that isn’t caught by the data validation you’re likely to be stuck with it… 4.2 Reproducibility - start as you mean to finish Now that you have your data, the temptation is to open an R script and get analyzing, or worse yet, open your raw data file in a spreadsheeting programme and crack out a few simple graphs… WAIT!!! Never do data analyses in your raw data files!!! These are your raw data, and they should stay that way. In fact, they should be set to be read-only so that you can’t edit them even if you wanted to. If you were to edit these files directly and make a mistake, like say sort a column without sorting the rest of the table, that’s it! Alles is kaput! You need your raw data files so that you have somewhere to look when you suspect you’ve done something bad to your data. Beyond data management, there’s code management, and version control software like GitHub is your friend, providing a backup of all your scripts, versioning (i.e. a record of changes of your scripts that you can revert should you need to - i.e. no more scripts named “analysis_final_final_FINAL.R”) and is a great tool for collaboration and sharing. In fact, GitHub can manage much more than just code, and can host your manuscript, output figures and tables, and even small datasets. Here’s a quick start guide for GitHub that shows you the basic functionality in ~10 minutes. I highly recommend working through it before you move on to the next steps… The trick to reproducible research is to start as you mean to finish. By this I mean that it is much easier to produce a reproducible research workflow if you work reproducibly from the start, rather than retrofitting a bunch of data and code you’ve already analyzed. This also means that you can reap the benefits of reproducibility during your project, which includes things like easier finding and fixing of errors, easier collaboration, simple splitting of the workflow if you decide that the project could lead to multiple publications, etc. The next 2 sections are covered in the quick start guide you should have worked through, but I add a few specific comments, and are essentially what you need to do to get set up with a GitHub repo before you get coding. 4.2.1 Creating a Git repository (or repo for short) Go to your online GitHub account (e.g. https://github.com/jslingsby) Click on the + in the top-right and select New repository Give your repository a name and description Choose whether you want the repo to be public or private (e.g. if you’re going to include sensitive data or otherwise don’t want the world to see what you’re doing) I usually Add a README file, that allows you to write a description of the project, explain the contents of the repo, etc I also usually Add .gitignore and choose R from the templates. This is a file that allows you to list files and folders that you don’t want GitHub to sync e.g. you may want to have a “bigdata” folder on your local machine with files too large to sync to GitHub (~&gt;50MB), although check out Git Large File Storage, or an “output” folder with large figures etc that you can easily reproduce from the code Add a license if you want too Click Create repository 4.2.2 Cloning your repo to your local machine Open RStudio on your laptop (you should already have RStudio and GitHub set up as per the instructions in section 1.4) Copy the URL to the repo you just created(e.g. https://github.com/jslingsby/demo) In RStudio: in the top-right of the window you’ll see Project: (None) - click on it and select New Project then Version Control then Git In the Repository URl: box paste the URL to your repo Project directory name should fill automatically For Create project as subdirectory of: hit Browse and navigate through your file system to where you want to put your folder for storing your Git repositories. I recommend something like ~Documents/GIT (If you’ve used Git before you may have set this already and can skip this step) Click Create Repository You should now see the name of your project/repo in the top-right corner of RStudio where previously you saw Project: (None) and you’re good to go. Now that you’re working on your local machine you need to set up a folder structure for code, output, data etc (using RStudio or your file explorer). Then you can open an R script, or better yet, an RMarkdown document that allows you to write text and embed code and images etc. 4.2.3 Keeping your online Git repository in sync Once you’ve added some content to your project (the local version of the Git repository), you want to sync it back to the online repository… From the “Git” tab (top-right window in RStudio), click the check boxes next to the files and folders you’ve created. This is equivalent to git add if you were doing this command line in bash or terminal. Click Commit, enter a log message (notes to yourself or collaborators about what you’re adding), and click Commit again. This is equivalent to git commit in command line. Now you’re going to “push” the changes to the online repository, but first, it’s always good practice to “pull” any changes from the repository to your local machine first. You do this, because if it was a collaborative project, others may have made changes since you last “pulled”. You may also have made changes to the repo directly in the website… To “pull” changes into RStudio, click the green down arrow (next to “Commit”). If it says “Already up-to-date” you can then “push” your changes up to Github by clicking on the green up arrow. This is equivalent to git push. 4.3 Tidy data and data wrangling Now you’re finally ready to start interrogating your data! But the raw data probably aren’t in a particularly analysis-friendly format, so you need to start with some data tidying and wrangling. You could do this “manually” in a new, appropriately named, copy of your raw data spreadsheet, but then you may not be able to track the changes you’ve made and errors may creep in. Where possible, it’s best to do this step with code. Most of my projects start with a “00_data_cleaning.R” script or similar to read the raw data and create an analysis-ready dataset. If this script takes a long time to run, I may output a new “clean data” file (or files) so that I don’t need to rerun the script every time. As you know from the readings for the course, there are many reasons why you should keep your data in tidy format (Wickham 2014). Unfortunately, as my spreadsheet above demonstrates, it’s not always convenient to enter your data in tidy format, so you inevitably end out needing to do post-entry data formatting. A major problem here is that doing the reformatting manually creates opportunities for introducing errors. This is where it’s a major advantage if you know how to wrangle your data in a coding language like R or Python. This is not to say that coded data wrangling cannot introduce errors!!! You should always do sanity checks!!! But I’d suspect that most of the errors you can make with code will be very obvious, because the outcome is usually spectacularly different to the input. The rest of this chapter provides a demonstration of some of the functionality of the tidyverse set of R packages, and some of the more common data wrangling errors made in R. 4.3.1 The dataset This tutorial relies on the BIO3018F prac data mentioned previously. See here if you’d like more details about the data and practical. 4.3.2 The motivation… Here’s a cool trick. Don’t freak out that I haven’t commented the code with explanations. We’ll break this down in the following sections. library(tidyverse) library(readxl) read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) %&gt;% mutate(CanopyCover = 100 - 4.16*Densiometer) %&gt;% select(!c(&quot;SoilColour&quot;, &quot;Densiometer&quot;)) %&gt;% pivot_longer(cols = c(&quot;PercentBareSoil&quot;, &quot;SoilPH&quot;, &quot;Dung&quot;, &quot;CanopyCover&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% ggplot() + geom_boxplot(aes(y = value, x = Site)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + facet_wrap(vars(variable), scales = &quot;free&quot;) In just 9 lines of code (and it could be fewer if I was being less neat) I read in the raw data with read_xlsx, created a new column that’s a transformation of an existing one with mutate, dropped unwanted variables with select (I didn’t actually need to do this, I was just showing off…), reshaped the entire dataframe from wide format (not tidy) to long format (tidy) with pivot_longer and made a pretty boxplot (ggplot + geom_boxplot) with rotated x-axis text (theme) and separate facets for each environmental variable (facet_wrap). You too can have these superhero powers! The rest of this chapter is focused on reading in data and various tricks to wrangle it into different formats, join separate tables, etc. Plotting you will have to learn yourself, but I may show a few simple examples. 4.4 Reading in data R has a number of different packages to read in data, and there’s even a few that work with different tabular data types just within the tidyverse. These include: readr for text files with values separated with tab, comma, or some other delimiter (.txt, .csv, .tsv file extensions) readxl for Microsoft Excel files (.xls or .xlsx) googlesheets4 for reading data directly from a Google Sheet There’s others too that can scrape data from web pages, interact with web APIs, or query local or online databases. Fortunately, within the tidyverse, similar syntax can be used across most data import packages. Check out the cheat sheet. 4.4.1 Read Google Sheets with googlesheets4 Since I gave you the link to a Google Sheet above, you should be able to read the data directly into R using that URL with the googlesheets4 package. Note that it may ask you to authenticate your Google account. You can do this with your personal or UCT account. # First let&#39;s call the libraries with the functions we want to use library(tidyverse) library(googlesheets4) # while googlesheets4 is part of the tidyverse, and installed when you install the tidyverse, it isn&#39;t a core package, so you have to call it separately # Now let&#39;s have a look at the names of the worksheets (tabs) within hte spreadsheet sheet_names(&quot;https://docs.google.com/spreadsheets/d/1U9yNaJFyd6kb5vlKzHhcgM6sLEzw_xVtQxewpH7k0Ho/edit?usp=sharing&quot;) ## [1] &quot;Sites&quot; &quot;Traits&quot; &quot;Species&quot; &quot;RangeLimits&quot; &quot;RangeSpecies&quot; Sometimes it shows lots of garble about the online authentication, but finally it shows us that there are three worksheets in the spreadsheet. Let’s read in the “Sites” sheet and call it edat for “environmental data”, and then print a summary. WARNING - running the name of a data object in R is usually a bad idea. It typically then shows you every value in the dataset, which can be hundreds of thousands of values! Tidyverse packages just show you a summary and the first few rows of data, which is nice… edat &lt;- read_sheet(&quot;https://docs.google.com/spreadsheets/d/1U9yNaJFyd6kb5vlKzHhcgM6sLEzw_xVtQxewpH7k0Ho/edit?usp=sharing&quot;, sheet = &quot;Sites&quot;) # read in data ## ✔ Reading from &quot;pracdatasheet&quot;. ## ✔ Range &#39;&#39;Sites&#39;&#39;. edat # print a summary of the data ## # A tibble: 24 × 8 ## Site Point SitePoint PercentBareSoil SoilPH SoilColour Dung Densiometer ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass SE grass_SE 0 4.6 7.5YR3/2 0 15 ## 2 renosterveld SE renosterveld_SE 5 5.4 10YR3/2 3 14 ## 3 invasion SE invasion_SE 0 3.79 10YR2/1 0 0 ## 4 sand SE sand_SE 10 4.82 10YR6/4 0 5 ## 5 sandstone SE sandstone_SE 5 4.91 10YR3/2 0 2 ## 6 limestone SE limestone_SE 0 6.48 10YR2/1 0 9 ## 7 grass NE grass_NE 0 5.13 10YR2/2 0 13 ## 8 renosterveld NE renosterveld_NE 30 5.7 7.5YR4/3 2 20 ## 9 invasion NE invasion_NE 5 3.7 10YR5/1 0 0 ## 10 sand NE sand_NE 8 4.54 10YR4/3 0 4 ## # … with 14 more rows 4.4.2 Read Microsoft Excel files with readxl Reading data in directly from Google Sheets can be super handy, but it requires an internet connection, and can become quite slow if your dataset gets big. In that case, it’s very easy to download Google Sheets as Microsoft Excel files and read them in using functions in the readxl package. First, we may want to know what files are in our data folder, which we can do with the base R function list.files(). Note that this function can be very handy if you have lots of files to process, like photos, etc. NOTE: you need to replace the working directory folder path (“data” here) with wherever you put the data on your computer. # get list of files in the folder (change to your own) list.files(&quot;data&quot;) ## [1] &quot;pracdatasheet.xlsx&quot; &quot;Reproducibility Survey Raw Data.xlsx&quot; Let’s have a look at the pracdatasheet.xlsx file library(tidyverse) library(readxl) #while readxl is part of the tidyverse and installed when you install the tidyverse, it isn&#39;t a core package, so you have to call it separately # See what sheets are in the Excel workbook excel_sheets(&quot;data/pracdatasheet.xlsx&quot;) ## [1] &quot;Sites&quot; &quot;Traits&quot; &quot;Species&quot; &quot;RangeLimits&quot; &quot;RangeSpecies&quot; # Read in data edat &lt;- read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) # Print a summary edat ## # A tibble: 24 × 8 ## Site Point SitePoint PercentBareSoil SoilPH SoilColour Dung Densiometer ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass SE grass_SE 0 4.6 7.5YR3/2 0 15 ## 2 renosterveld SE renosterveld_SE 5 5.4 10YR3/2 3 14 ## 3 invasion SE invasion_SE 0 3.79 10YR2/1 0 0 ## 4 sand SE sand_SE 10 4.82 10YR6/4 0 5 ## 5 sandstone SE sandstone_SE 5 4.91 10YR3/2 0 2 ## 6 limestone SE limestone_SE 0 6.48 10YR2/1 0 9 ## 7 grass NE grass_NE 0 5.13 10YR2/2 0 13 ## 8 renosterveld NE renosterveld_NE 30 5.7 7.5YR4/3 2 20 ## 9 invasion NE invasion_NE 5 3.7 10YR5/1 0 0 ## 10 sand NE sand_NE 8 4.54 10YR4/3 0 4 ## # … with 14 more rows A handy feature of read_sheet and read_xlsx is the range = option, which allows you to select only the regions of the worksheet you want using the usual syntax you’d use in Excel, e.g. A1:Z100 would give you columns A to Z and rows 1 to 100. Here’s an example skipping the first two columns: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;, range = &quot;C1:H25&quot;) ## # A tibble: 24 × 6 ## SitePoint PercentBareSoil SoilPH SoilColour Dung Densiometer ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass_SE 0 4.6 7.5YR3/2 0 15 ## 2 renosterveld_SE 5 5.4 10YR3/2 3 14 ## 3 invasion_SE 0 3.79 10YR2/1 0 0 ## 4 sand_SE 10 4.82 10YR6/4 0 5 ## 5 sandstone_SE 5 4.91 10YR3/2 0 2 ## 6 limestone_SE 0 6.48 10YR2/1 0 9 ## 7 grass_NE 0 5.13 10YR2/2 0 13 ## 8 renosterveld_NE 30 5.7 7.5YR4/3 2 20 ## 9 invasion_NE 5 3.7 10YR5/1 0 0 ## 10 sand_NE 8 4.54 10YR4/3 0 4 ## # … with 14 more rows NOTE: I’m just doing this for demonstration purposes. This particular dataset is already a rectangular table (or data frame), so it would be easiest to just read the whole thing in and subset the columns you want with select() and the rows with filter(), both from the dplyr package and a core part of tidyverse - demonstrated later. You can also be sneaky and read multiple separate chunks and then stitch them together, e.g. Reading in different columns and binding them together: bind_cols(read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;, range = &quot;A1:B25&quot;), read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;, range = &quot;G1:H25&quot;)) ## # A tibble: 24 × 4 ## Site Point Dung Densiometer ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass SE 0 15 ## 2 renosterveld SE 3 14 ## 3 invasion SE 0 0 ## 4 sand SE 0 5 ## 5 sandstone SE 0 2 ## 6 limestone SE 0 9 ## 7 grass NE 0 13 ## 8 renosterveld NE 2 20 ## 9 invasion NE 0 0 ## 10 sand NE 0 4 ## # … with 14 more rows Or by row: bind_rows(read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;, range = &quot;C1:H15&quot;), read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;, range = &quot;C16:H25&quot;)) ## # A tibble: 23 × 12 ## SitePoint PercentBareSoil SoilPH SoilColour Dung Densiometer invasion_SW `10.0` `3.93` `5Y2.5/1` `0.0` `3.0` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass_SE 0 4.6 7.5YR3/2 0 15 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 2 renosterveld_SE 5 5.4 10YR3/2 3 14 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 3 invasion_SE 0 3.79 10YR2/1 0 0 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 4 sand_SE 10 4.82 10YR6/4 0 5 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 5 sandstone_SE 5 4.91 10YR3/2 0 2 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 6 limestone_SE 0 6.48 10YR2/1 0 9 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 7 grass_NE 0 5.13 10YR2/2 0 13 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 8 renosterveld_NE 30 5.7 7.5YR4/3 2 20 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 9 invasion_NE 5 3.7 10YR5/1 0 0 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## 10 sand_NE 8 4.54 10YR4/3 0 4 &lt;NA&gt; NA NA &lt;NA&gt; NA NA ## # … with 13 more rows Note that the skip = and nmax = options are somewhat related and tell the function the number of rows to skip before reading anything and the maximum number of rows to read respectively. 4.4.3 Reading text files with readr I’m not going to go through this package, but it’s important to note that both Microsoft Excel and Google Sheets are proprietary. In other words these are not open formats and are not necessarily stable in the long term. YOU SHOULD NOT STORE YOUR DATA IN THESE FORMATS, and as such they should not be used if you want your workflow to be reproducible! The safest is to save each worksheet from the spreadsheet as separate comma or tab delimited text files, which you can read in with functions from readr like read_csv or read_delim. Try ?readr for details on the package and functions. 4.5 A quick aside on data types You’ll note &lt;chr&gt; and &lt;dbl&gt; in the output when we print a summary of the data. These indicate that the data types (or classes) for those columns are “character” and “double” respectively… R’s basic data types are character, integer, numeric (or double, they are synonymous), date and logical. There are a number of other special types, but this covers most of those you’ll commonly deal with. integer is for whole numbers (i.e. no decimals) numeric or double (double precision floating point numbers) are real numbers - i.e. they include decimal places. For reasons I won’t explain, storing data as real numbers takes up far more memory than integers. Obviously, lots of numbers we work with are real numbers, so we need to be able to work with them. Interestingly, large data storage and transmission projects (e.g. satellite remote sensing) often do tricks like multiply values by some constant so that all data stored are integers. You need to correct by the constant during your analysis to get values in the actual unit of measurement. character is for letters, words, sentences or mixed strings (i.e. letters and digits) factor is a set of levels (often treatments) that you’d use in an analysis. They are very useful, but they are also VERY likely to trip you up at some stage (I’ll explain why in a minute)!!! factors are essentially numerical codings of character data in some order. Using mumerical coding is useful, because you can order your levels in an analysis - e.g. Treatment A, Treatment B, Control, etc. They are also useful, because they can store data efficiently. For example, if your analysis has thousands of data points, it uses much less memory to recode the treatments above as 1, 2, 3, etc and just store one copy of the levels (what we call the labels in a factors). logical is the outcome of a conditional statement (i.e. values can only be TRUE or FALSE) it is actually just a special case of factor data where FALSE = 0 and TRUE = 1, so if you take the sum of a vector of logical values, it will give you a count of the number of TRUE cases. That said, R treats logical in specific ways that can’t be done with true factors and vice versa, so forget I said that. date once again is a special case of a factor, but again, R treats dates in specific ways that it wouldn’t treat a normal factor. Dates are thus stored as numbers, where each day has a unique value. For R, by default the numbering begins at the 1st January 1970 (the “origin”), which is coded as “0” - try run the code as.numeric(as.Date(\"1970-01-01\")), which coerced the date “1970-01-01” to a number. You can change the origin to whatever you prefer, and this can be very important, because different software use different origins… Here’re some code examples to try to drive this home, using the built in vector `letters letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; You can check the class of data using the function class() class(letters) ## [1] &quot;character&quot; And typically convert (or coerce) between classes using as. followed by the class you want, e.g. as.factor(letters) ## [1] a b c d e f g h i j k l m n o p q r s t u v w x y z ## Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z Here it shows us that we have a factor with 26 values with 26 unique levels. You wouldn’t really use a factor class if all values are unique though. Here’s a better example. as.factor(rep(letters[1:3], 8)) ## [1] a b c a b c a b c a b c a b c a b c a b c a b c ## Levels: a b c And to prove that the factor represents the levels as a set of numbers: as.numeric(as.factor(rep(letters[1:3], 8))) ## [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 Note that this doesn’t work if we try to coerce the the letters to numbers without making them a factor first: as.numeric(rep(letters[1:3], 8)) ## Warning: NAs introduced by coercion ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA Here’s how factors will catch you out, sooner or later… As I said earlier, storing data as a factor can be less memory intensive. R takes advantage of this in that some functions (especially when reading in data) try to identify the most sensible data type and store it in R’s memory as such. So for example, if all the valuesin a column of a table are whole numbers, R will read it in as class integer, which requires less memory than class numeric (or double). By the same token, if the column is text and some values are repeated, R will often make it class factor. This is great, but it can catch you out as follows. If a column of numbers has any text in it, R will make the whole column class character. If many of the values are repeated, it may even make it class factor for efficiency. This happens a lot!, especially if someone makes an error and types in an O instead of a 0, or denotes missing data with a “.” or anything else other than “NA”. The problem is that even though R typically orders factors in alphabetical or numerical order, it is possible to create a mismatch between the actual number and the order in the levels. Here are some code examples. Firstly, where a number is treated as a character. hmm &lt;- rep(c(4, &quot;0&quot;, 2, 5, 3, 1), 3) hmm ## [1] &quot;4&quot; &quot;0&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; &quot;4&quot; &quot;0&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; &quot;4&quot; &quot;0&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; class(hmm) ## [1] &quot;character&quot; In this case the vector is class character, but if we’d read it in from a file it would likely have been made a factor. Let’s see what happens when we coerce it to factor: as.factor(hmm) ## [1] 4 0 2 5 3 1 4 0 2 5 3 1 4 0 2 5 3 1 ## Levels: 0 1 2 3 4 5 Seems ok? What if we try to coerce that factor to a number? as.numeric(as.factor(hmm)) ## [1] 5 1 3 6 4 2 5 1 3 6 4 2 5 1 3 6 4 2 Ouch! While the level labels start at 0, the numeric representation of the levels start at 1, so coercing them to numbers creates a mismatch… If you need to fix this, you should always coerce to character first. as.numeric(as.character(as.factor(hmm))) ## [1] 4 0 2 5 3 1 4 0 2 5 3 1 4 0 2 5 3 1 What about if we insert a character instead of a number, like “O” instead of “0” hmm2 &lt;- rep(c(4, &quot;O&quot;, 2, 5, 3, 1), 3) hmm2 ## [1] &quot;4&quot; &quot;O&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; &quot;4&quot; &quot;O&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; &quot;4&quot; &quot;O&quot; &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;1&quot; class(hmm2) ## [1] &quot;character&quot; as.factor(hmm2) ## [1] 4 O 2 5 3 1 4 O 2 5 3 1 4 O 2 5 3 1 ## Levels: 1 2 3 4 5 O Interesting, the “O” is now the 6th level, not the first… What if we coerce to numeric? as.numeric(as.factor(hmm2)) ## [1] 4 6 2 5 3 1 4 6 2 5 3 1 4 6 2 5 3 1 Oops!!! Converting all your zeroes to sixes is really going to mess with your results!!! And if we try our as.character fix? as.numeric(as.character(as.factor(hmm2))) ## Warning: NAs introduced by coercion ## [1] 4 NA 2 5 3 1 4 NA 2 5 3 1 4 NA 2 5 3 1 R doesn’t know what to do with the Os, so it makes them NA and gives you a warning. Watch out for this, because ignoring the warning and leaving out zero values could seriously bias your results! 4.6 Learning to speak Tidyverse The tidyverse packages contain a series of functions that can be thought of as verbs, providing the grammar for you develop code statements. For example, some of the main functions in library(dplyr) are: arrange() - Arrange rows by the values of column select() - Select a subset of columns using their names and types filter() - Subset rows using column values slice() - Subset rows using their positions mutate() - Create, modify, and delete columns group_by() - Group by one or more variables summarise() - Summarise each group to fewer rows (Pro tip: Do yourself a favour and spend some time reading about these functions!) Binding the verbs together into code “sentences” like the example I gave in section 4.3.2 is made possible by the pipe %&gt;% operator, which essentially allows you to pass the output of one function (or verb) to the next without making a new object in memory each time. i.e. without %&gt;% the code in section 4.3.2 would have required me to save 4 intermediate objects into R’s memory as I went. This is clunky, and wastes RAM (computer memory space), often slowing the script down. Here are some example “sentences” using some of the dplyr “verbs”: “Take the environmental data, group it by the Site labels and calculate the means for a specific set of variables for each group.” edat %&gt;% group_by(Site) %&gt;% summarize(BareSoil = mean(PercentBareSoil), `Soil pH` = mean(SoilPH), Dung = mean(Dung)) ## # A tibble: 6 × 4 ## Site BareSoil `Soil pH` Dung ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass 2.5 4.85 0.25 ## 2 invasion 5 3.88 0.5 ## 3 limestone 30 6.33 0 ## 4 renosterveld 21.2 5.47 2.5 ## 5 sand 12 4.81 0.5 ## 6 sandstone 31.2 4.76 0.5 Note how you can create variables with new names when you summarise, and that you don’t have to apply the same summary function to each (mean in this case, but you can use median, sd, etc). Also notice how enclosing names in “`” allows you to have spaces or special characters in the variable name. “Take the environmental data, filter it for only the renosterveld site, and select a specific set of variables.” edat %&gt;% filter(Site == &quot;renosterveld&quot;) %&gt;% select(Point, PercentBareSoil, SoilPH, Dung) ## # A tibble: 4 × 4 ## Point PercentBareSoil SoilPH Dung ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SE 5 5.4 3 ## 2 NE 30 5.7 2 ## 3 SW 20 5.43 3 ## 4 NW 30 5.35 2 “Take the environmental data, filter it for only the renosterveld site, select a specific set of variables, and add a new column that expresses the frequency of dung observed as a proportion of the percentage bare soil exposed.” edat %&gt;% filter(Site == &quot;renosterveld&quot;) %&gt;% select(Point, PercentBareSoil, SoilPH, Dung) %&gt;% mutate(DungPerSoil = Dung/PercentBareSoil) ## # A tibble: 4 × 5 ## Point PercentBareSoil SoilPH Dung DungPerSoil ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SE 5 5.4 3 0.6 ## 2 NE 30 5.7 2 0.0667 ## 3 SW 20 5.43 3 0.15 ## 4 NW 30 5.35 2 0.0667 “Take the environmental data, select a specific set of variables, add a new column that expresses the frequency of dung observed as a proportion of the percentage bare soil exposed, and arrange the samples in descending order of the new variable.” edat %&gt;% select(SitePoint, PercentBareSoil, SoilPH, Dung) %&gt;% mutate(DungPerSoil = Dung/PercentBareSoil) %&gt;% arrange(desc(DungPerSoil)) ## # A tibble: 24 × 5 ## SitePoint PercentBareSoil SoilPH Dung DungPerSoil ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass_SW 0 5.11 1 Inf ## 2 renosterveld_SE 5 5.4 3 0.6 ## 3 invasion_NW 5 4.1 2 0.4 ## 4 sand_NW 10 4.95 2 0.2 ## 5 renosterveld_SW 20 5.43 3 0.15 ## 6 renosterveld_NE 30 5.7 2 0.0667 ## 7 renosterveld_NW 30 5.35 2 0.0667 ## 8 sandstone_SW 30 4.7 1 0.0333 ## 9 sandstone_NW 45 4.7 1 0.0222 ## 10 sand_SE 10 4.82 0 0 ## # … with 14 more rows I think you get the picture… 4.7 Getting things Tidy Ok, most of what I’ve shown you is about reading in data and then manipulating it once you have it read in. While this is handy, there are a few more functions you’ll need to help get your data Tidy. Specifically, Tidy data is where: Every column is variable. Every row is an observation. Every cell is a single value. There are many ways to violate this, but some of the most common are: When your data are in “wide” format When you have data in your column names (often associated with wide format) When you have multiple variables in a single column (or cell) When you have gaps in your data 4.7.1 pivot_longer Perhaps the most useful function here is pivot_longer from the tidyr package, which allows you to convert wide format data to long format. Here’s our raw data: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) ## # A tibble: 24 × 8 ## Site Point SitePoint PercentBareSoil SoilPH SoilColour Dung Densiometer ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 grass SE grass_SE 0 4.6 7.5YR3/2 0 15 ## 2 renosterveld SE renosterveld_SE 5 5.4 10YR3/2 3 14 ## 3 invasion SE invasion_SE 0 3.79 10YR2/1 0 0 ## 4 sand SE sand_SE 10 4.82 10YR6/4 0 5 ## 5 sandstone SE sandstone_SE 5 4.91 10YR3/2 0 2 ## 6 limestone SE limestone_SE 0 6.48 10YR2/1 0 9 ## 7 grass NE grass_NE 0 5.13 10YR2/2 0 13 ## 8 renosterveld NE renosterveld_NE 30 5.7 7.5YR4/3 2 20 ## 9 invasion NE invasion_NE 5 3.7 10YR5/1 0 0 ## 10 sand NE sand_NE 8 4.54 10YR4/3 0 4 ## # … with 14 more rows Here’s our data in “long” format: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) %&gt;% pivot_longer(cols = c(&quot;PercentBareSoil&quot;, &quot;SoilPH&quot;, &quot;Dung&quot;, &quot;Densiometer&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) ## # A tibble: 96 × 6 ## Site Point SitePoint SoilColour variable value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 grass SE grass_SE 7.5YR3/2 PercentBareSoil 0 ## 2 grass SE grass_SE 7.5YR3/2 SoilPH 4.6 ## 3 grass SE grass_SE 7.5YR3/2 Dung 0 ## 4 grass SE grass_SE 7.5YR3/2 Densiometer 15 ## 5 renosterveld SE renosterveld_SE 10YR3/2 PercentBareSoil 5 ## 6 renosterveld SE renosterveld_SE 10YR3/2 SoilPH 5.4 ## 7 renosterveld SE renosterveld_SE 10YR3/2 Dung 3 ## 8 renosterveld SE renosterveld_SE 10YR3/2 Densiometer 14 ## 9 invasion SE invasion_SE 10YR2/1 PercentBareSoil 0 ## 10 invasion SE invasion_SE 10YR2/1 SoilPH 3.79 ## # … with 86 more rows Note 1: I had to leave out the variable SoilColour, because it was a different data type - &lt;chr&gt; as opposed to &lt;dbl&gt;. I could only combine them into the same column if I convered them all to a common data type, which would have to be character in this case, and would not be very useful… Note 2: The columns that you don’t select with the cols = argument just get repeated throughout the data, so be careful when analyzing them because there will be duplicates… 4.7.2 pivot_wider You can also convert long format data to wide format when needed using pivot_wider. You may need to do this when working with biological community data, because most analysis software and R functions prefer “community data matrices” with species as columns and sites as rows. This violates Tidy data, because you have data in your column names (i.e. species names). I’ll demonstrate this using the Species worksheet from our dataset. First, let’s look at the raw data: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) ## # A tibble: 419 × 12 ## Site Point Photo Family Genus Species AddOn FieldName FullName Worki…¹ Alien FullID ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 invasion SE NA Asparagaceae Asparagus &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Asparagus Aspara… NA NA ## 2 invasion SE NA Poaceae Capeochloa arundinacea &lt;NA&gt; &lt;NA&gt; Capeochlo… Capeoc… NA NA ## 3 invasion SE NA Proteaceae Leucadendron salignum &lt;NA&gt; &lt;NA&gt; Leucadend… Leucad… NA 1 ## 4 invasion SE NA Poaceae &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Small grass &lt;NA&gt; Small … NA NA ## 5 invasion SE NA Ericaceae Erica &lt;NA&gt; small invasion &lt;NA&gt; Erica sma… Erica … NA NA ## 6 invasion SE NA Rhamnaceae Phylica ericoides &lt;NA&gt; &lt;NA&gt; Phylica e… Phylic… NA 1 ## 7 invasion SE NA Fabaceae Acacia saligna &lt;NA&gt; &lt;NA&gt; Acacia sa… Acacia… 1 1 ## 8 invasion SE NA Cyperaceae Schoenus &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Schoenus Schoen… NA NA ## 9 invasion SE NA Pinaceae Pinus &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Pinus Pinus 1 NA ## 10 invasion SE NA Fabaceae Acacia pycnantha &lt;NA&gt; &lt;NA&gt; Acacia py… Acacia… 1 1 ## # … with 409 more rows, and abbreviated variable name ¹​WorkingName A bit messy, but in this case it is presence-only data and all we want are “Site”, “Point” and “WorkingName”, so if we just select those we’ll be close to having our long format data, like so: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% select(Site, Point, WorkingName) ## # A tibble: 419 × 3 ## Site Point WorkingName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 invasion SE Asparagus ## 2 invasion SE Capeochloa arundinacea ## 3 invasion SE Leucadendron salignum ## 4 invasion SE Small grass ## 5 invasion SE Erica small invasion ## 6 invasion SE Phylica ericoides ## 7 invasion SE Acacia saligna ## 8 invasion SE Schoenus ## 9 invasion SE Pinus ## 10 invasion SE Acacia pycnantha ## # … with 409 more rows And we can make it wider like so: read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% select(Site, Point, WorkingName) %&gt;% mutate(Presence = 1) %&gt;% # adds a column called &quot;Presence&quot; filled with &quot;1&quot; pivot_wider(names_from = WorkingName, values_from = Presence, values_fill = 0) ## # A tibble: 24 × 186 ## Site Point Aspar…¹ Capeo…² Leuca…³ Small…⁴ Erica…⁵ Phyli…⁶ Acaci…⁷ Schoe…⁸ Pinus Acaci…⁹ Acaci…˟ Platy…˟ Resti…˟ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 invasion SE 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 2 renoster… SW 0 0 1 0 0 0 0 0 0 0 0 0 0 ## 3 grass NE 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 grass NW 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 grass SE 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 grass SW 1 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 renoster… NE 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 8 renoster… NW 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 9 renoster… SE 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 10 invasion SW 0 1 0 0 0 0 1 1 1 0 0 0 1 ## # … with 14 more rows, 171 more variables: `Helichrysum crispa` &lt;dbl&gt;, `Cynodon dactylon` &lt;dbl&gt;, `Briza major` &lt;dbl&gt;, ## # Metalasia &lt;dbl&gt;, Thesium &lt;dbl&gt;, `Helichrysum rosum` &lt;dbl&gt;, `Sporobolus africanus` &lt;dbl&gt;, Rhus &lt;dbl&gt;, ## # Eragrostis &lt;dbl&gt;, Oedera &lt;dbl&gt;, `Ehrharta calycina` &lt;dbl&gt;, `Pennisetum clandestinum` &lt;dbl&gt;, Carex &lt;dbl&gt;, ## # `Aspalathus yellow furry` &lt;dbl&gt;, Lobelia &lt;dbl&gt;, `Osteospermum monilifera` &lt;dbl&gt;, Prismatocarpus &lt;dbl&gt;, ## # `Seriphium plumosum` &lt;dbl&gt;, `Dicerothamnus rhinocerotis` &lt;dbl&gt;, `Rhus glauca` &lt;dbl&gt;, Senecio &lt;dbl&gt;, ## # Pentameris &lt;dbl&gt;, `Senecio burchellii` &lt;dbl&gt;, `Trifolium angustifolium` &lt;dbl&gt;, Aristida &lt;dbl&gt;, ## # `Athanasia trifurcata` &lt;dbl&gt;, `Physalis viscosa` &lt;dbl&gt;, MustardPod &lt;dbl&gt;, Eucalyptus &lt;dbl&gt;, … Note the use of mutate to add a column of ones. You could also use add_column. WARNING! Note that if you have multiple identical rows in your data then pivot_wider won’t work without a little extra info. In this case, the data are presence only, so there should be no duplicates, but if it were catch data for example, you could have duplicates, because you may have the same species caught multiple times at one site. In this case you either need to group_by site and species and use summarize to take the sum before you pivot_wider, or you can add a function like sum to pivot_wider using the values_fn = argument. See ?pivot_wider for details. Ok, looks good, but one thing we’re missing here is that our sites are currently identified by the combination of both the Site and Point columns, whereas community data matrices usually need to have a single unique name for each site. Fortunately, there are tidyverse functions for dealing with this. 4.7.3 unite and separate Here I use unite to combine the Site and Point columns into a new column called SitePoint. I’ve also changed the select line to only keep our new column, so that we output a community data matrix. read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% unite(&quot;SitePoint&quot;, Site:Point, sep = &quot;_&quot;) %&gt;% select(SitePoint, WorkingName) %&gt;% mutate(Presence = 1) %&gt;% # adds a column called &quot;Presence&quot; filled with &quot;1&quot; pivot_wider(names_from = WorkingName, values_from = Presence, values_fill = 0) ## # A tibble: 24 × 185 ## SiteP…¹ Aspar…² Capeo…³ Leuca…⁴ Small…⁵ Erica…⁶ Phyli…⁷ Acaci…⁸ Schoe…⁹ Pinus Acaci…˟ Acaci…˟ Platy…˟ Resti…˟ Helic…˟ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 invasi… 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 2 renost… 0 0 1 0 0 0 0 0 0 0 0 0 0 1 ## 3 grass_… 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 4 grass_… 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 grass_… 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 grass_… 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 renost… 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 8 renost… 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 9 renost… 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 10 invasi… 0 1 0 0 0 0 1 1 1 0 0 0 1 0 ## # … with 14 more rows, 170 more variables: `Cynodon dactylon` &lt;dbl&gt;, `Briza major` &lt;dbl&gt;, Metalasia &lt;dbl&gt;, ## # Thesium &lt;dbl&gt;, `Helichrysum rosum` &lt;dbl&gt;, `Sporobolus africanus` &lt;dbl&gt;, Rhus &lt;dbl&gt;, Eragrostis &lt;dbl&gt;, Oedera &lt;dbl&gt;, ## # `Ehrharta calycina` &lt;dbl&gt;, `Pennisetum clandestinum` &lt;dbl&gt;, Carex &lt;dbl&gt;, `Aspalathus yellow furry` &lt;dbl&gt;, ## # Lobelia &lt;dbl&gt;, `Osteospermum monilifera` &lt;dbl&gt;, Prismatocarpus &lt;dbl&gt;, `Seriphium plumosum` &lt;dbl&gt;, ## # `Dicerothamnus rhinocerotis` &lt;dbl&gt;, `Rhus glauca` &lt;dbl&gt;, Senecio &lt;dbl&gt;, Pentameris &lt;dbl&gt;, ## # `Senecio burchellii` &lt;dbl&gt;, `Trifolium angustifolium` &lt;dbl&gt;, Aristida &lt;dbl&gt;, `Athanasia trifurcata` &lt;dbl&gt;, ## # `Physalis viscosa` &lt;dbl&gt;, MustardPod &lt;dbl&gt;, Eucalyptus &lt;dbl&gt;, `Athanasia juncea` &lt;dbl&gt;, `Osyris compressa` &lt;dbl&gt;, … You can also split values using separate. For example, you may want to split the Latin names into genus and species. read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% separate(WorkingName, c(&quot;Gen&quot;, &quot;Sp&quot;)) %&gt;% select(Gen, Sp) ## Warning: Expected 2 pieces. Additional pieces discarded in 5 rows [5, 29, 145, 223, 401]. ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 58 rows [1, 8, 9, 12, 17, 18, 21, 23, 24, 28, 30, 32, 34, 37, 38, ## 41, 45, 48, 49, 50, ...]. ## # A tibble: 419 × 2 ## Gen Sp ## &lt;chr&gt; &lt;chr&gt; ## 1 Asparagus &lt;NA&gt; ## 2 Capeochloa arundinacea ## 3 Leucadendron salignum ## 4 Small grass ## 5 Erica small ## 6 Phylica ericoides ## 7 Acacia saligna ## 8 Schoenus &lt;NA&gt; ## 9 Pinus &lt;NA&gt; ## 10 Acacia pycnantha ## # … with 409 more rows Note the warnings, because some plants were identified to genus only, or had makeshift names with more than 2 words like “Little yellow daisy”. 4.8 Getting out of the tidyverse Pro Tip… Before I move on, there are times when you may need to get your data out of the tidyverse formats. Specifically, many R packages that analyze biological community data (e.g. library(vegan)) require community data matrices with column and row names. For some reason that I have yet to understand, the tidyverse tabular data class (tibble) doesn’t believe in row names, so to use your community data matrix with vegan etc, you have to break free of the tidyverse, which can be surprisingly difficult, but here’s how. # First rerun our tidyverse code to get as close to a community data matrix as possible comm &lt;- read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% unite(&quot;SitePoint&quot;, Site:Point, sep = &quot;_&quot;) %&gt;% select(SitePoint, WorkingName) %&gt;% mutate(Presence = 1) %&gt;% # adds a column called &quot;Presence&quot; filled with &quot;1&quot; pivot_wider(names_from = WorkingName, values_from = Presence, values_fill = 0) # Then some Untidyverse code to comm &lt;- as.data.frame(unclass(comm)) # get out of the tibble format rownames(comm) &lt;- comm[,1] # add the first column &quot;SitePoint&quot; as rownames comm &lt;- comm[,-1] # remove the first column (i.e. &quot;SitePoint&quot;) comm[1:5, 1:3] # view the first 5 rows and 3 columns ## Asparagus Capeochloa.arundinacea Leucadendron.salignum ## invasion_SE 1 1 1 ## renosterveld_SW 0 0 1 ## grass_NE 1 0 0 ## grass_NW 0 0 0 ## grass_SE 1 0 0 For more on analyzing community data, check out this tutorial. 4.9 Joining dataframes Often it’s easiest to work with multiple different data tables, like the Species and Site worksheets we’ve been playing with, but there are times that you’d like to compare a variable from one with a variable from another, and to do this you need to join them. For example, let’s compare the count of species for each site with some of the environmental variables. First, we calculate the number of species recorded in each site and save it as an object sr. sr &lt;- read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Species&quot;) %&gt;% unite(&quot;SitePoint&quot;, Site:Point, sep = &quot;_&quot;) %&gt;% select(SitePoint, WorkingName) %&gt;% group_by(SitePoint) %&gt;% summarize(`Species Number` = n()) sr ## # A tibble: 24 × 2 ## SitePoint `Species Number` ## &lt;chr&gt; &lt;int&gt; ## 1 grass_NE 17 ## 2 grass_NW 12 ## 3 grass_SE 13 ## 4 grass_SW 14 ## 5 invasion_NE 15 ## 6 invasion_NW 10 ## 7 invasion_SE 13 ## 8 invasion_SW 8 ## 9 limestone_NE 17 ## 10 limestone_NW 23 ## # … with 14 more rows Now let’s read in our site data, selet the columns we want and join sr to it using left_join. read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) %&gt;% select(SitePoint, PercentBareSoil, SoilPH) %&gt;% left_join(sr, by = &quot;SitePoint&quot;) ## # A tibble: 24 × 4 ## SitePoint PercentBareSoil SoilPH `Species Number` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 grass_SE 0 4.6 13 ## 2 renosterveld_SE 5 5.4 18 ## 3 invasion_SE 0 3.79 13 ## 4 sand_SE 10 4.82 27 ## 5 sandstone_SE 5 4.91 23 ## 6 limestone_SE 0 6.48 26 ## 7 grass_NE 0 5.13 17 ## 8 renosterveld_NE 30 5.7 17 ## 9 invasion_NE 5 3.7 15 ## 10 sand_NE 8 4.54 18 ## # … with 14 more rows Hey presto! But what was it we wanted to compare? Well why not be unscientific and just plot everything against everything else using a function from another library called GGally. read_xlsx(&quot;data/pracdatasheet.xlsx&quot;, sheet = &quot;Sites&quot;) %&gt;% select(SitePoint, PercentBareSoil, SoilPH) %&gt;% left_join(sr, by = &quot;SitePoint&quot;) %&gt;% GGally::ggpairs(columns = 2:ncol(.)) A weak correlation between soil pH and % bare soil, and a near-significant (0.05 &lt; p &lt; 0.1) correlation between soil pH and species number. Note that if you only plan to use functions from a library a few times in your script it can be more efficient to call the function from the library once off using the syntax library::function (e.g. GGally::ggpairs) than to attach the whole library (i.e. calling library(GGally)) as it saves RAM. That’s it! Go forth and conquer! References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
